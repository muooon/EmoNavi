import torch
from torch.optim import Optimizer
import math
from typing import Tuple, Callable, Union

# Helper function (Lynx)
def exists(val):
    return val is not None

class EmoLynx(Optimizer):
    # „ÇØ„É©„ÇπÂÆöÁæ©ÔºÜÂàùÊúüÂåñ
    def __init__(self, params: Union[list, torch.nn.Module], lr=1e-3, betas=(0.9, 0.99), 
    # lynxÁî®„Éô„Éº„ÇøÔΩ•‰∫íÊèõÊÄß„ÅÆËøΩÂä†(lynxÁî®beta1ÔΩ•beta2)
                 eps=1e-8, weight_decay=0.01, decoupled_weight_decay: bool = False): 

        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)
        
        # lynx„Å´Âøú„Åò„Å¶„Ç¶„Çß„Ç§„ÉàÊ∏õË°∞„ÅÆ„Åü„ÇÅ‰øùÂ≠ò
        self._init_lr = lr
        self.decoupled_wd = decoupled_weight_decay
        self.should_stop = False # ÂÅúÊ≠¢„Éï„É©„Ç∞„ÅÆÂàùÊúüÂåñ

    # ÊÑüÊÉÖEMAÊõ¥Êñ∞(Á∑äÂºµ„Å®ÂÆâÈùô)
    def _update_ema(self, state, loss_val):
        ema = state.setdefault('ema', {})
        ema['short'] = 0.3 * loss_val + 0.7 * ema.get('short', loss_val)
        ema['long']  = 0.01 * loss_val + 0.99 * ema.get('long', loss_val)
        return ema

    # ÊÑüÊÉÖ„Çπ„Ç´„É©„ÉºÂÄ§ÁîüÊàê(EMAÂ∑ÆÂàÜ„ÄÅÊªë„Çâ„Åã„Å™ÈùûÁ∑öÂΩ¢„Çπ„Ç´„É©„Éº„ÄÅtanh 5 * diff „ÅßÈã≠Êïè„ÅïÂº∑Ë™ø)
    def _compute_scalar(self, ema):
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    # ShadowÊ∑∑ÂêàÊØîÁéá(> 0.6Ôºö70„Äú90%„ÄÅ < 0.6Ôºö10%„ÄÅ > 0.3Ôºö30%„ÄÅ Âπ≥ÊôÇÔºö0%)
    def _decide_ratio(self, scalar):
        if scalar > 0.6:
            return 0.7 + 0.2 * scalar
        elif scalar < -0.6:
            return 0.1
        elif abs(scalar) > 0.3:
            return 0.3
        return 0.0

    # ÊêçÂ§±ÂèñÂæó(ÊêçÂ§±ÂÄ§ loss_val „ÇíÊï∞ÂÄ§Âåñ„ÄÅÊÑüÊÉÖÂà§ÂÆö„Å´‰ΩøÁî®„ÄÅÂ≠òÂú®„Åó„Å™„ÅÑ„Éë„É©„É°„Éº„Çø(Êõ¥Êñ∞‰∏çË¶Å)„ÅØ„Çπ„Ç≠„ÉÉ„Éó)
    @torch.no_grad()
    def step(self, closure: Callable | None = None): # „ÇØ„É≠„Éº„Ç∏„É£„ÅÆÂûã„Éí„É≥„Éà„ÇíËøΩÂä†
        loss = None
        if exists(closure): # ‰∏ÄË≤´ÊÄß„ÅÆ„Åü„ÇÅ„Å´exists„Éò„É´„Éë„Éº„Çí‰Ωø„ÅÜ
            with torch.enable_grad():
                loss = closure()
        loss_val = loss.item() if loss is not None else 0.0

        for group in self.param_groups:
            # „É™„É≥„ÇØ„ÇπÂÖ±ÈÄö„Éë„É©„É°„Éº„ÇøÊäΩÂá∫
            lr, wd, beta1, beta2 = group['lr'], group['weight_decay'], *group['betas']
            
            # „Ç¶„Çß„Ç§„ÉàÊ∏õË°∞„ÅÆÂá¶ÁêÜ„ÇíÂàÜÈõ¢ (from lynx)
            _wd_actual = wd
            if self.decoupled_wd:
                _wd_actual /= self._init_lr # ÈùûÈÄ£ÁµêÊôÇ„Ç¶„Çß„Ç§„ÉàÊ∏õË°∞Ë™øÊï¥

            for p in filter(lambda p: exists(p.grad), group['params']): # PG„ÉÅ„Çß„ÉÉ„ÇØ„Å´„Éï„Ç£„É´„Çø

                grad = p.grad # PGÁõ¥Êé•‰ΩøÁî®(Ë®àÁÆó„Å´".data"‰∏çË¶Å)
                state = self.state[p]

                # EMAÊõ¥Êñ∞„Éª„Çπ„Ç´„É©„ÉºÁîüÊàê(EMAÂ∑ÆÂàÜ„Åã„Çâ„Çπ„Ç´„É©„Éº„ÇíÁîüÊàê„Åó„Çπ„Éë„Ç§„ÇØÊØîÁéá„ÇíÊ±∫ÂÆö)
                ema = self._update_ema(state, loss_val)
                scalar = self._compute_scalar(ema)
                ratio = self._decide_ratio(scalar)

                # shadow_paramÔºöÂøÖË¶ÅÊôÇ„ÅÆ„ÅøÊõ¥Êñ∞(„Çπ„Éë„Ç§„ÇØÈÉ®ÂàÜ„Å´ÁèæÂú®ÂÄ§„Çí5%„Åö„Å§ËøΩÂæì„Åï„Åõ„ÇãÂãïÁöÑÂ±•Ê≠¥)
                if ratio > 0:
                    if 'shadow' not in state:
                        state['shadow'] = p.data.clone()
                    else:
                        p.data.mul_(1 - ratio).add_(state['shadow'], alpha=ratio) 
                        state['shadow'].lerp_(p.data, 0.05) 
                        # lynxÊõ¥Êñ∞Ââç p.data „Åß shadow Êõ¥Êñ∞(ÁèæÂú®ÂÄ§„Çí5%„Åö„Å§ËøΩÂæì)
                        # p.data.mul_(1 - ratio).add_(state['shadow'], alpha=ratio) 
                        # EmoNavi: p.data = p.data * (1-ratio) + shadow * ratio

                # --- Start Lynx Gradient Update Logic ---
                
                # lynxÂàùÊúüÂåñ(exp_avg_sq)
                if 'exp_avg' not in state:
                    state['exp_avg'] = torch.zeros_like(p)
                exp_avg = state['exp_avg']

                # Stepweight decay (from lynx): p.data = p.data * (1 - lr * wd)
                # decoupled_wd ËÄÉÊÖÆ _wd_actual ‰ΩøÁî®(EmoNavi„ÅÆwd„ÅØÊúÄÂæå„Å´ÈÅ©Áî®)
                p.data.mul_(1. - lr * _wd_actual)

                # ÂãæÈÖç„Éñ„É¨„É≥„Éâ
                # m_t = beta1 * exp_avg_prev + (1 - beta1) * grad
                blended_grad = grad.mul(1. - beta1).add_(exp_avg, alpha=beta1)
                
                # p: p.data = p.data - lr * sign(blended_grad)
                p.data.add_(blended_grad.sign_(), alpha = -lr)

                # exp_avg = beta2 * exp_avg + (1 - beta2) * grad
                exp_avg.mul_(beta2).add_(grad, alpha = 1. - beta2)

                # --- End Lynx Gradient Update Logic ---

                # Early StopÁî® scalarË®òÈå≤(„Éê„ÉÉ„Éï„Ç°ÂÖ±ÈÄö„ÅßÁÆ°ÁêÜ/ÊúÄÂ§ß32‰ª∂‰øùÊåÅ/ÂãïÈùôË©ï‰æ°)
                # „Åì„ÅÆÈÉ®ÂàÜ„ÅØ p.state „Åß„ÅØ„Å™„Åè self.state „Å´„Ç¢„ÇØ„Çª„Çπ„Åô„Çã
                hist = self.state.setdefault('scalar_hist', [])
                hist.append(scalar)
                if len(hist) > 32:
                    hist.pop(0)

        # Early StopÂà§Êñ≠(Èùô„Åë„Åï„ÅÆÂêàÂõ≥) - This part is outside the inner loop
        if len(self.state['scalar_hist']) >= 32:
            buf = self.state['scalar_hist']
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True # üí° Â§ñÈÉ®„Åã„Çâ„Åì„Çå„ÇíË¶ã„Å¶Âà§Êñ≠ÂèØ

        return loss

"""
Lynx was developed with inspiration from Lion and Tiger, 
which we deeply respect for their lightweight and intelligent design.  
Lynx also integrates EmoNAVI to enhance its capabilities.
"""