import torch
from torch.optim import Optimizer
import math
from typing import Callable, Union, Dict, Any, Tuple

"""
EmoClan v2.0 (250815) shadow-system v2.0 scalar-switch v2.0
AMPå¯¾å¿œå®Œäº†(202507) p.data -> p ä¿®æ­£æ¸ˆã¿
memo : "optimizer = EmoClan(model.parameters(), lr=1e-3, use_shadow=True)"
optimizer æŒ‡å®šã®éš›ã« True ã«ã™ã‚‹ã“ã¨ã§ shadow ã‚’ã‚ªãƒ³ã«ã§ãã‚‹
emosens shadow-effect v1.0 åæ˜  shadow-systemã€scalar-switch ä¿®æ­£
"""

# Helper function
def exists(val):
    return val is not None

class EmoClan(Optimizer):
    # ã‚¯ãƒ©ã‚¹å®šç¾©ï¼†åˆæœŸåŒ– ğŸ”¸Shadow True(æœ‰åŠ¹)/False(ç„¡åŠ¹) åˆ‡æ›¿ãˆ
    def __init__(self, params: Union[list, torch.nn.Module], 
                 lr: float = 1e-3, 
                 betas: Tuple[float, float] = (0.9, 0.999), 
                 eps: float = 1e-8, 
                 weight_decay: float = 0.01,
                 lynx_betas: Tuple[float, float] = (0.9, 0.99), # Lynx å›ºæœ‰ã® beta
                 decoupled_weight_decay: bool = False,
                 use_shadow: bool = False
                ):
        
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 0: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta parameter at index 1: {betas[1]}")
        
        # Lynx ã® betas ã‚‚ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not 0.0 <= lynx_betas[0] < 1.0:
            raise ValueError(f"Invalid lynx_beta parameter at index 0: {lynx_betas[0]}")
        if not 0.0 <= lynx_betas[1] < 1.0:
            raise ValueError(f"Invalid lynx_beta parameter at index 1: {lynx_betas[1]}")

        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,
                        lynx_betas=lynx_betas, decoupled_weight_decay=decoupled_weight_decay)
        super().__init__(params, defaults)
        
        self._init_lr = lr # decoupled weight decay ã®ãŸã‚ã«ä¿å­˜ (Lynxç”¨)
        self.should_stop = False # å…¨ä½“ã®åœæ­¢ãƒ•ãƒ©ã‚°
        self.use_shadow = use_shadow # EmoClanã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è‡ªèº«ãŒuse_shadowã‚’ä¿æŒ

    # --- æ„Ÿæƒ…æ©Ÿæ§‹ (Emotion Mechanism) ---
    def _update_ema(self, param_state: Dict[str, Any], loss_val: float) -> Dict[str, float]:
        """æå¤±å€¤ã«åŸºã¥ã„ã¦çŸ­æœŸãƒ»é•·æœŸ EMA ã‚’æ›´æ–°"""
        # param_state ã¯å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® state['ema'] ã‚’ä¿æŒã™ã‚‹
        ema = param_state.setdefault('ema', {'short': loss_val, 'long': loss_val})
        ema['short'] = 0.3 * loss_val + 0.7 * ema['short']
        ema['long'] = 0.01 * loss_val + 0.99 * ema['long']
        return ema

    """EMA ã®å·®åˆ†ã‹ã‚‰æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’ç”Ÿæˆ"""
    def _compute_scalar(self, ema: Dict[str, float]) -> float:
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    """æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼ã«åŸºã¥ã„ã¦ Shadow ã®æ··åˆæ¯”ç‡ã‚’æ±ºå®š"""
    # Shadowæ··åˆæ¯”ç‡(> abs 0.6ï¼š60ã€œ100%ã€ > abs 0.1ï¼š10ã€œ60%ã€ å¹³æ™‚ï¼š0%) emosensåæ˜ 
    # æ—§ï¼šShadowæ··åˆæ¯”ç‡(> 0.6ï¼š80ã€œ90%ã€ < -0.6ï¼š10%ã€ abs> 0.3ï¼š30%ã€ å¹³æ™‚ï¼š0%)
    # èª¬æ˜ï¼šscalar>+0.6 ã¯ "return 0.7(é–‹å§‹å€¤) + 0.2(å¤‰åŒ–å¹…) * scalar" = 0.82ï½0.9 â† èª¤
    # ä¿®æ­£1ï¼šscalar>Â±0.6 ã‚’ "return é–‹å§‹å€¤ + (abs(scalar) - 0.6(ç¯„å›²)) / ç¯„å›²é‡ * å¤‰åŒ–å¹…"
    # ä¿®æ­£2ï¼šscalar>Â±0.1 ã‚’ "return é–‹å§‹å€¤ + (abs(scalar) - 0.1(ç¯„å›²)) / ç¯„å›²é‡ * å¤‰åŒ–å¹…"
    # ã‚¿ã‚¹ã‚¯ç­‰ã«å¿œã˜ãŸèª¿æ•´ã®ãŸã‚ï¼“æ®µéšã§é©ç”¨ã—ã¦ãŠã(ä¸Šè¨˜ã‚’å‚è€ƒã«èª¿æ•´ã—ã¦ãã ã•ã„ï¼ç¾çŠ¶ã¯shadow-effectåæ˜ )
    def _decide_ratio(self, scalar: float) -> float:
        if not self.use_shadow:
            return 0.0 # ğŸ”¸use_shadow ãŒ False ã®å ´åˆã¯å¸¸ã«æ¯”ç‡ã‚’ 0 ã«ã™ã‚‹
        if abs(scalar) > 0.6:
            return 0.6 + (abs(scalar) - 0.6) / 0.4 * 0.4 # å…ƒ return 0.7 + 0.2 * scalar
        elif abs(scalar) > 0.1:
            return 0.1 + (abs(scalar) - 0.1) / 0.5 * 0.5 # å…ƒ return 0.3
        return 0.0

    # --- å„æœ€é©åŒ–å™¨ã®ã‚³ã‚¢ãªå‹¾é…æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ (ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦çµ±åˆ) ---

    def _lynx_update(
        self, 
        p: torch.Tensor, 
        grad: torch.Tensor, 
        param_state: Dict[str, Any], 
        lr: float, 
        beta1: float, 
        beta2: float, 
        wd_actual: float
    ):
        """EmoLynx ã®ã‚³ã‚¢ãªå‹¾é…æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯"""
        # Stepweight decay: p = p * (1 - lr * wd)
        p.mul_(1. - lr * wd_actual)

        # Lynx å›ºæœ‰ã® EMA çŠ¶æ…‹ã¯ param_state ã«ä¿æŒ
        if 'exp_avg_lynx' not in param_state:
            param_state['exp_avg_lynx'] = torch.zeros_like(p)
        exp_avg = param_state['exp_avg_lynx']

        # å‹¾é…ãƒ–ãƒ¬ãƒ³ãƒ‰
        blended_grad = grad.mul(1. - beta1).add_(exp_avg, alpha=beta1)
        
        # ç¬¦å·ãƒ™ãƒ¼ã‚¹ã®æ›´æ–°
        p.add_(blended_grad.sign_(), alpha = -lr)

        # exp_avg æ›´æ–°
        exp_avg.mul_(beta2).add_(grad, alpha = 1. - beta2)

    def _navi_update(
        self, 
        p: torch.Tensor, 
        grad: torch.Tensor, 
        param_state: Dict[str, Any], 
        lr: float, 
        betas: Tuple[float, float], 
        eps: float, 
        weight_decay: float
    ):
        """EmoNavi ã®ã‚³ã‚¢ãªå‹¾é…æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯"""
        beta1, beta2 = betas

        exp_avg = param_state.setdefault('exp_avg_navi', torch.zeros_like(p))
        exp_avg_sq = param_state.setdefault('exp_avg_sq_navi', torch.zeros_like(p.to(torch.float32)))

        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
        exp_avg_sq.mul_(beta2).addcmul_(grad.to(torch.float32), grad.to(torch.float32), value=1 - beta2)
        denom = exp_avg_sq.sqrt().add_(eps)

        # Weight decay (æ¨™æº–çš„æ‰‹æ³•)
        if weight_decay:
            p.mul_(1 - lr * weight_decay) 

        p.addcdiv_(exp_avg, denom, value=-lr)

    def _fact_update(
        self, 
        p: torch.Tensor, 
        grad: torch.Tensor, 
        param_state: Dict[str, Any], 
        lr: float, 
        betas: Tuple[float, float], # beta2 ã¯ç¾çŠ¶ä½¿ã‚ã‚Œãªã„ãŒäº’æ›æ€§ã®ãŸã‚æ®‹ã™ (1Då‹¾é…ã§ä½¿ç”¨)
        eps: float, 
        weight_decay: float
    ):
        """EmoFact ã®ã‚³ã‚¢ãªå‹¾é…æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ (Adafactor ãƒ©ã‚¤ã‚¯)"""
        beta1, beta2 = betas

        if grad.dim() >= 2:
            # è¡Œã¨åˆ—ã®2ä¹—å¹³å‡ã‚’è¨ˆç®— (åˆ†æ•£ã®è»½é‡ãªè¿‘ä¼¼)
            # gradã‚’float32ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦è¨ˆç®—ã™ã‚‹ã“ã¨ã§æ•°å€¤å®‰å®šæ€§ã‚’é«˜ã‚ã‚‹
            r_sq = torch.mean(grad.to(torch.float32) * grad.to(torch.float32), dim=tuple(range(1, grad.dim())), keepdim=True).add_(eps)
            c_sq = torch.mean(grad.to(torch.float32) * grad.to(torch.float32), dim=0, keepdim=True).add_(eps)

            param_state.setdefault('exp_avg_r_fact', torch.zeros_like(r_sq)).mul_(beta1).add_(torch.sqrt(r_sq), alpha=1 - beta1)
            param_state.setdefault('exp_avg_c_fact', torch.zeros_like(c_sq)).mul_(beta1).add_(torch.sqrt(c_sq), alpha=1 - beta1)
            
            # å†æ§‹ç¯‰ã—ãŸè¿‘ä¼¼å‹¾é…ã®å¹³æ–¹æ ¹ã®ç©ã§æ­£è¦åŒ–
            denom = torch.sqrt(param_state['exp_avg_r_fact'] * param_state['exp_avg_c_fact']).add_(eps)
            update_term = grad / denom # grad ã¯å…ƒã®å‹ï¼ˆfloat16ã¾ãŸã¯float32ï¼‰

        else: # 1æ¬¡å…ƒ(ãƒ™ã‚¯ãƒˆãƒ«)ã®å‹¾é…è£œæ­£
            exp_avg = param_state.setdefault('exp_avg_fact', torch.zeros_like(p))
            exp_avg_sq = param_state.setdefault('exp_avg_sq_fact', torch.zeros_like(p.to(torch.float32)))
            
            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
            exp_avg_sq.mul_(beta2).addcmul_(grad.to(torch.float32), grad.to(torch.float32), value=1 - beta2)
            denom = exp_avg_sq.sqrt().add_(eps)
            update_term = exp_avg / denom

        # æœ€çµ‚çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° (decoupled weight decayã‚‚é©ç”¨)
        # decoupled_weight_decay ã¯ __init__ ã§ã‚°ãƒ«ãƒ¼ãƒ—ã«defaultsã¨ã—ã¦æ¸¡ã•ã‚Œã¦ã„ã‚‹ãŒã€
        # ã“ã“ã§ã¯factorãƒ­ã‚¸ãƒƒã‚¯è‡ªä½“ãŒweight_decayã‚’å—ã‘å–ã‚‹å½¢å¼
        p.mul_(1 - weight_decay * lr) 
        p.add_(update_term, alpha=-lr)


    @torch.no_grad()
    def step(self, closure: Callable | None = None):
        loss = None
        if exists(closure):
            with torch.enable_grad():
                loss = closure()
        loss_val = loss.item() if loss is not None else 0.0

        # å…¨ä½“ã® scalar_hist ã‚’ EmoClan ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ç®¡ç†
        global_scalar_hist = self.state.setdefault('global_scalar_hist', [])
        
        # å…¨ä½“ã¨ã—ã¦ã®æ„Ÿæƒ…EMAçŠ¶æ…‹ã‚’ self.state ã«ä¿æŒã—ã€ç¾åœ¨ã®æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¨ˆç®—
        global_ema_state = self.state.setdefault('global_ema', {'short': loss_val, 'long': loss_val})
        global_ema_state['short'] = 0.3 * loss_val + 0.7 * global_ema_state['short']
        global_ema_state['long'] = 0.01 * loss_val + 0.99 * global_ema_state['long']
        current_global_scalar = self._compute_scalar(global_ema_state)
        
        # global_scalar_hist ã«ç¾åœ¨ã®æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼ã‚’è¿½åŠ 
        global_scalar_hist.append(current_global_scalar)
        if len(global_scalar_hist) >= 33:
            global_scalar_hist.pop(0)


        for group in self.param_groups:
            lr = group['lr']
            wd = group['weight_decay']
            eps = group['eps']
            decoupled_wd = group['decoupled_weight_decay']
            
            lynx_beta1, lynx_beta2 = group['lynx_betas']
            navi_fact_betas = group['betas'] # Navi/Fact å…±é€šã® beta ã‚’ä½¿ç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® betas)
            
            # Lynx ã® decoupled_wd ã®ãŸã‚ã® _wd_actual è¨ˆç®—
            _wd_actual_lynx = wd
            if decoupled_wd:
                _wd_actual_lynx /= self._init_lr

            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                param_state = self.state[p] # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨ã®çŠ¶æ…‹

                # --- å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨ã®æ„Ÿæƒ…æ©Ÿæ§‹ã®æ›´æ–°ã¨ Shadow å‡¦ç† ---
                # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® state['ema'] ã¯ã€ãã‚Œãã‚Œã® loss_val (å…¨ä½“ã§å…±é€š) ã‚’å…ƒã«æ›´æ–°ã•ã‚Œã‚‹
                # ãŸã ã—ã€ç¾çŠ¶ã® loss_val ã¯ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã‹ã‚‰å—ã‘å–ã£ãŸå˜ä¸€ã®å€¤ãªã®ã§ã€
                # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å›ºæœ‰ã®ã€Œæ„Ÿæƒ…ã€ã‚’å®šç¾©ã™ã‚‹ã‚ˆã‚Šã€å…¨ä½“ã¨ã—ã¦ã®æ„Ÿæƒ…ãŒä½¿ã‚ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã€‚
                # use_shadow ãŒ True ã®å ´åˆã«ã®ã¿ Shadow é–¢é€£ã®å‡¦ç†ã‚’å®Ÿè¡Œ
                if self.use_shadow:  
                    param_ema = self._update_ema(param_state, loss_val) 
                    param_scalar = self._compute_scalar(param_ema) # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å›ºæœ‰ã®ã‚¹ã‚«ãƒ©ãƒ¼

                    ratio = self._decide_ratio(param_scalar) # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å›ºæœ‰ã® ratio

                    if ratio > 0:
                        if 'shadow' not in param_state:
                            param_state['shadow'] = p.clone()
                        else:
                            # Shadow ã‚’ç¾åœ¨å€¤ã«ãƒ–ãƒ¬ãƒ³ãƒ‰
                            p.mul_(1 - ratio).add_(param_state['shadow'], alpha=ratio)
                        # Shadow ã‚’ç¾åœ¨å€¤ã«è¿½å¾“ã•ã›ã‚‹
                        param_state['shadow'].lerp_(p, 0.05)

                # --- æœ€é©åŒ–å™¨ã®é¸æŠã¨å‹¾é…æ›´æ–° ---
                # ç¾åœ¨ã®global_scalar_histã«è¨˜éŒ²ã•ã‚ŒãŸå…¨ä½“ã¨ã—ã¦ã®æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼ã«åŸºã¥ã„ã¦ãƒ•ã‚§ãƒ¼ã‚ºã‚’åˆ¤æ–­
                # global_scalar > abs 0.6 ã®ç¯„å›²ã¯ Lynx
                # global_scalar > abs 0.3 ã®ç¯„å›²ã¯ Fact
                # global_scalar < abs 0.3 ã®ç¯„å›²ã¯ Navi
                if abs(current_global_scalar) > 0.6: # åºç›¤ãƒ»éå­¦ç¿’ãƒ»ç™ºæ•£æ™‚
                    self._lynx_update(p, grad, param_state, lr, lynx_beta1, lynx_beta2, _wd_actual_lynx)
                elif abs(current_global_scalar) > 0.3: # çµ‚ç›¤ãƒ»éå­¦ç¿’ãƒ»ç™ºæ•£å‚¾å‘æ™‚
                    self._fact_update(p, grad, param_state, lr, navi_fact_betas, eps, wd)
                else: # -0.3 <= current_global_scalar <= 0.3 ã®ä¸­ç›¤ï½¥å¹³æ™‚(å®‰å®šæœŸ)
                    self._navi_update(p, grad, param_state, lr, navi_fact_betas, eps, wd)

        # Early Stopåˆ¤æ–­
        # global_scalar_hist ã®è©•ä¾¡
        if len(global_scalar_hist) >= 32:
            buf = global_scalar_hist
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True # å¤–éƒ¨ã‹ã‚‰ã“ã‚Œã‚’è¦‹ã¦åˆ¤æ–­å¯

        return loss

"""
 Emoã‚·ãƒªãƒ¼ã‚ºã¯ã€Adamã€Adafactorã€Lionã€Tigerã€ç­‰ã‹ã‚‰å¤šãã‚’å­¦ã³ã¾ã—ãŸã€‚  
 ã“ã®é–‹ç™ºã«ãŠã„ã¦å…ˆäººãŸã¡ã®çŸ¥è¦‹ã«æ·±ãæ„Ÿè¬ã—ã¤ã¤ä»Šå¾Œã‚‚æ–°ã—ã„å¯èƒ½æ€§ã‚’æ¢ç©¶ã—ã¾ã™ã€‚ 
 The Emo series has learned much from Adam, Adafactor, Lion, and Tiger.  
 Rather than being their successors,  
 In its development, we deeply appreciate the insights of those who came before usâ€”and continue to explore new possibilities beyond them.  
"""