import torch
from torch.optim import Optimizer
import math
from typing import Tuple, Callable, Union

"""
EmoNeco v2.0 (250815) shadow-system v2.0 scalar-switch v2.0
AMPå¯¾å¿œå®Œäº†(202507) p.data -> p ä¿®æ­£æ¸ˆã¿
memo : "optimizer = EmoNeco(model.parameters(), lr=1e-3, use_shadow=True)"
optimizer æŒ‡å®šã®éš›ã« True ã«ã™ã‚‹ã“ã¨ã§ shadow ã‚’ã‚ªãƒ³ã«ã§ãã‚‹
emosens shadow-effect v1.0 åæ˜  shadow-systemã€scalar-switch ä¿®æ­£
"""

# Helper function (Lynx)
def exists(val):
    return val is not None
# Soft Sign é–¢æ•°
def softsign(x):
    return x / (1 + x.abs())

class EmoNeco(Optimizer):
    # ã‚¯ãƒ©ã‚¹å®šç¾©ï¼†åˆæœŸåŒ– ğŸ”¸Shadow True(æœ‰åŠ¹)/False(ç„¡åŠ¹) åˆ‡æ›¿ãˆ
    def __init__(self, params: Union[list, torch.nn.Module], lr=1e-3, betas=(0.9, 0.99), 
    # necoç”¨ãƒ™ãƒ¼ã‚¿ï½¥äº’æ›æ€§ã®è¿½åŠ (necoç”¨beta1ï½¥beta2)
                 eps=1e-8, weight_decay=0.01, decoupled_weight_decay: bool = False, use_shadow: bool = False): 

        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)
        
        # ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°ã®ãŸã‚ä¿å­˜
        self._init_lr = lr
        self.decoupled_wd = decoupled_weight_decay
        self.should_stop = False # åœæ­¢ãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–
        self.use_shadow = use_shadow # ğŸ”¸shadowã®ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’ä¿å­˜

    # æ„Ÿæƒ…EMAæ›´æ–°(ç·Šå¼µã¨å®‰é™)
    def _update_ema(self, state, loss_val):
        ema = state.setdefault('ema', {})
        ema['short'] = 0.3 * loss_val + 0.7 * ema.get('short', loss_val)
        ema['long'] = 0.01 * loss_val + 0.99 * ema.get('long', loss_val)
        return ema

    # æ„Ÿæƒ…ã‚¹ã‚«ãƒ©ãƒ¼å€¤ç”Ÿæˆ(EMAå·®åˆ†ã€æ»‘ã‚‰ã‹ãªéç·šå½¢ã‚¹ã‚«ãƒ©ãƒ¼ã€tanh 5 * diff ã§é‹­æ•ã•å¼·èª¿)
    def _compute_scalar(self, ema):
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    # Shadowæ··åˆæ¯”ç‡(> abs 0.6ï¼š60ã€œ100%ã€ > abs 0.1ï¼š10ã€œ60%ã€ å¹³æ™‚ï¼š0%) emosensåæ˜ 
    # æ—§ï¼šShadowæ··åˆæ¯”ç‡(> 0.6ï¼š80ã€œ90%ã€ < -0.6ï¼š10%ã€ abs> 0.3ï¼š30%ã€ å¹³æ™‚ï¼š0%)
    # èª¬æ˜ï¼šscalar>+0.6 ã¯ "return 0.7(é–‹å§‹å€¤) + 0.2(å¤‰åŒ–å¹…) * scalar" = 0.82ï½0.9 â† èª¤
    # ä¿®æ­£1ï¼šscalar>Â±0.6 ã‚’ "return é–‹å§‹å€¤ + (abs(scalar) - 0.6(ç¯„å›²)) / ç¯„å›²é‡ * å¤‰åŒ–å¹…"
    # ä¿®æ­£2ï¼šscalar>Â±0.1 ã‚’ "return é–‹å§‹å€¤ + (abs(scalar) - 0.1(ç¯„å›²)) / ç¯„å›²é‡ * å¤‰åŒ–å¹…"
    # ã‚¿ã‚¹ã‚¯ç­‰ã«å¿œã˜ãŸèª¿æ•´ã®ãŸã‚ï¼“æ®µéšã§é©ç”¨ã—ã¦ãŠã(ä¸Šè¨˜ã‚’å‚è€ƒã«èª¿æ•´ã—ã¦ãã ã•ã„ï¼ç¾çŠ¶ã¯shadow-effectåæ˜ )
    def _decide_ratio(self, scalar):
        if not self.use_shadow:
            return 0.0 # ğŸ”¸use_shadow ãŒ False ã®å ´åˆã¯å¸¸ã«æ¯”ç‡ã‚’ 0 ã«ã™ã‚‹
        if abs(scalar) > 0.6:
            return 0.6 + (abs(scalar) - 0.6) / 0.4 * 0.4 # å…ƒ return 0.7 + 0.2 * scalar
        elif abs(scalar) > 0.1:
            return 0.1 + (abs(scalar) - 0.1) / 0.5 * 0.5 # å…ƒ return 0.3
        return 0.0

    # æå¤±å–å¾—(æå¤±å€¤ loss_val ã‚’æ•°å€¤åŒ–ã€æ„Ÿæƒ…åˆ¤å®šã«ä½¿ç”¨ã€å­˜åœ¨ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(æ›´æ–°ä¸è¦)ã¯ã‚¹ã‚­ãƒƒãƒ—)
    @torch.no_grad()
    def step(self, closure: Callable | None = None): # ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£ã®å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        loss = None
        if exists(closure): # ä¸€è²«æ€§ã®ãŸã‚ã«existsãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’ä½¿ã†
            with torch.enable_grad():
                loss = closure()
        loss_val = loss.item() if loss is not None else 0.0

        for group in self.param_groups:
            # å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
            lr, wd, beta1, beta2 = group['lr'], group['weight_decay'], *group['betas']
            
            # ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°ã®å‡¦ç†ã‚’åˆ†é›¢ (from lynx)
            _wd_actual = wd
            if self.decoupled_wd:
                _wd_actual /= self._init_lr # éé€£çµæ™‚ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°èª¿æ•´

            for p in filter(lambda p: exists(p.grad), group['params']): # PGãƒã‚§ãƒƒã‚¯ã«ãƒ•ã‚£ãƒ«ã‚¿

                grad = p.grad # PGç›´æ¥ä½¿ç”¨(è¨ˆç®—ã«".data"ä¸è¦)
                state = self.state[p]

                # EMAæ›´æ–°ãƒ»ã‚¹ã‚«ãƒ©ãƒ¼ç”Ÿæˆ(EMAå·®åˆ†ã‹ã‚‰ã‚¹ã‚«ãƒ©ãƒ¼ã‚’ç”Ÿæˆã—ã‚¹ãƒ‘ã‚¤ã‚¯æ¯”ç‡ã‚’æ±ºå®š)
                ema = self._update_ema(state, loss_val)
                scalar = self._compute_scalar(ema)
                ratio = self._decide_ratio(scalar) # ğŸ”¸use_shadow ã«å¿œã˜ã¦ ratio ãŒ 0 ã«ãªã‚‹

                # shadow_paramï¼šå¿…è¦æ™‚ã®ã¿æ›´æ–°(ã‚¹ãƒ‘ã‚¤ã‚¯éƒ¨åˆ†ã«ç¾åœ¨å€¤ã‚’5%ãšã¤è¿½å¾“ã•ã›ã‚‹å‹•çš„å±¥æ­´)
                # ğŸ”¸self.use_shadow ãŒ True ã§ã€ã‹ã¤ ratio > 0 ã®å ´åˆã®ã¿ shadow ã‚’æ›´æ–°
                if self.use_shadow and ratio > 0: 
                    if 'shadow' not in state:
                        state['shadow'] = p.clone()
                    else:
                        p.mul_(1 - ratio).add_(state['shadow'], alpha=ratio) 
                        state['shadow'].lerp_(p, 0.05) 
                        # æ›´æ–°å‰ p ã§ shadow æ›´æ–°(ç¾åœ¨å€¤ã‚’5%ãšã¤è¿½å¾“)
                        # p.mul_(1 - ratio).add_(state['shadow'], alpha=ratio) 
                        # EmoNavi: p = p * (1-ratio) + shadow * ratio

                # --- Start Neco Gradient Update Logic ---
                
                # necoåˆæœŸåŒ–(exp_avg_sq)
                if 'exp_avg' not in state:
                    state['exp_avg'] = torch.zeros_like(p)
                exp_avg = state['exp_avg']

                # Stepweight decay (from lynx): p = p * (1 - lr * wd)
                # decoupled_wd è€ƒæ…® _wd_actual ä½¿ç”¨(EmoNaviã®wdã¯æœ€å¾Œã«é©ç”¨)
                p.mul_(1. - lr * _wd_actual)

                # å‹¾é…ãƒ–ãƒ¬ãƒ³ãƒ‰
                # m_t = beta1 * exp_avg_prev + (1 - beta1) * grad
                blended_grad = grad.mul(1. - beta1).add_(exp_avg, alpha=beta1)
                grad_norm = torch.norm(grad, dtype=torch.float32) # å‹¾é…ãƒãƒ«ãƒ ã®è¨ˆç®—

                # å‰Šé™¤ï¼š-0.2 < scalar <= -0.5 : SoftSign (ã‚†ã£ãã‚Šæ»‘ã‚‰ã‹ã«)
                #  0.2 < abs(scalar) <=  0.5 : SoftSign+norm (æºã‚Œã‚’æ»‘ã‚‰ã‹ã«)
                # ãã‚Œä»¥å¤– Cautious (å¹³æ™‚ã‚„éé©åˆã‚„å´©å£Šå‚¾å‘ã‚’æ…é‡ã«)
                # p - lr * softsign(blended_grad) (from softsign)
                # p - lr * direction * mask (from Cautious)
                # safe_norm æ¥µå€¤ã®ãƒ–ãƒ¬ãƒ³ãƒ‰å‹¾é…ã«å¯¾ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                if 0.2 < abs(scalar) <= 0.5:
                    safe_norm = grad_norm + eps
                    modified_grad = softsign(blended_grad) * safe_norm
                    p.add_(-lr * modified_grad) 
                else:
                    direction = blended_grad.sign()    # å‹¾é…æ–¹å‘ã®ç¬¦å· Cautious å‡¦ç†
                    mask = (direction == grad.sign())  # éå»ã®å‹¾é…ã¨æ–¹å‘ãŒä¸€è‡´ã—ã¦ã„ã‚‹éƒ¨åˆ†ã®ã¿æ›´æ–°
                    p.add_(direction * mask, alpha = -lr)  # Cautious æ›´æ–°

                # exp_avg = beta2 * exp_avg + (1 - beta2) * grad
                exp_avg.mul_(beta2).add_(grad, alpha = 1. - beta2)

                # --- End Neco Gradient Update Logic ---

                # Early Stopç”¨ scalarè¨˜éŒ²(ãƒãƒƒãƒ•ã‚¡å…±é€šã§ç®¡ç†/æœ€å¤§32ä»¶ä¿æŒ/å‹•é™è©•ä¾¡)
                # ã“ã®éƒ¨åˆ†ã¯ p.state ã§ã¯ãªã self.state ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹
                hist = self.state.setdefault('scalar_hist', [])
                hist.append(scalar)
                if len(hist) >= 33:
                    hist.pop(0)

        # Early Stopåˆ¤æ–­(é™ã‘ã•ã®åˆå›³) This part is outside the inner loop
        if len(self.state['scalar_hist']) >= 32:
            buf = self.state['scalar_hist']
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True # å¤–éƒ¨ã‹ã‚‰ã“ã‚Œã‚’è¦‹ã¦åˆ¤æ–­å¯

        return loss

"""
 https://github.com/muooon/EmoNavi
 Neco was developed with inspiration from Lion, Tiger, Cautious, softsign, and EmoLynx 
 which we deeply respect for their lightweight and intelligent design.  
 Neco also integrates EmoNAVI to enhance its capabilities.
"""