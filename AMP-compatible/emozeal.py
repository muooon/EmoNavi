import torch
from torch.optim import Optimizer
import math

"""
EmoZeal v2.0 (250815) shadow-system v2.0 scalar-switch v2.0
AMPÂØæÂøúÂÆå‰∫Ü(202507) p.data -> p ‰øÆÊ≠£Ê∏à„Åø
memo : "optimizer = EmoNeco(model.parameters(), lr=1e-3, use_shadow=True)"
optimizer ÊåáÂÆö„ÅÆÈöõ„Å´ True „Å´„Åô„Çã„Åì„Å®„Åß shadow „Çí„Ç™„É≥„Å´„Åß„Åç„Çã
emosens shadow-effect v1.0 ÂèçÊò† shadow-system„ÄÅscalar-switch ‰øÆÊ≠£
"""

# Soft Sign Èñ¢Êï∞
def softsign(x): 
    return x / (1 + x.abs())
    
class EmoZeal(Optimizer):
    # „ÇØ„É©„ÇπÂÆöÁæ©ÔºÜÂàùÊúüÂåñ üî∏Shadow True(ÊúâÂäπ)/False(ÁÑ°Âäπ) ÂàáÊõø„Åà
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999),
                 eps=1e-8, weight_decay=0.01, use_shadow: bool = False): 
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)

        super().__init__(params, defaults)

        self.alpha_prev = getattr(self, 'alpha_prev', 1.0)
        self._init_lr = lr 
        self.should_stop = False # ÂÅúÊ≠¢„Éï„É©„Ç∞„ÅÆÂàùÊúüÂåñ
        self.use_shadow = use_shadow # üî∏shadow„ÅÆ‰ΩøÁî®„Éï„É©„Ç∞„Çí‰øùÂ≠ò

    # ÊÑüÊÉÖEMAÊõ¥Êñ∞(Á∑äÂºµ„Å®ÂÆâÈùô)
    def _update_ema(self, state, loss_val):
        ema = state.setdefault('ema', {})
        ema['short'] = 0.3 * loss_val + 0.7 * ema.get('short', loss_val)
        ema['long'] = 0.01 * loss_val + 0.99 * ema.get('long', loss_val)
        return ema

    # ÊÑüÊÉÖ„Çπ„Ç´„É©„ÉºÂÄ§ÁîüÊàê(EMAÂ∑ÆÂàÜ„ÄÅÊªë„Çâ„Åã„Å™ÈùûÁ∑öÂΩ¢„Çπ„Ç´„É©„Éº„ÄÅtanh 5 * diff „ÅßÈã≠Êïè„ÅïÂº∑Ë™ø)
    def _compute_scalar(self, ema):
        diff = ema['short'] - ema['long']
        return math.tanh(5 * diff)

    # ShadowÊ∑∑ÂêàÊØîÁéá(> abs 0.6Ôºö60„Äú100%„ÄÅ > abs 0.1Ôºö10„Äú60%„ÄÅ Âπ≥ÊôÇÔºö0%) emosensÂèçÊò†
    # ÊóßÔºöShadowÊ∑∑ÂêàÊØîÁéá(> 0.6Ôºö80„Äú90%„ÄÅ < -0.6Ôºö10%„ÄÅ abs> 0.3Ôºö30%„ÄÅ Âπ≥ÊôÇÔºö0%)
    # Ë™¨ÊòéÔºöscalar>+0.6 „ÅØ "return 0.7(ÈñãÂßãÂÄ§) + 0.2(Â§âÂåñÂπÖ) * scalar" = 0.82ÔΩû0.9 ‚Üê Ë™§
    # ‰øÆÊ≠£1Ôºöscalar>¬±0.6 „Çí "return ÈñãÂßãÂÄ§ + (abs(scalar) - 0.6(ÁØÑÂõ≤)) / ÁØÑÂõ≤Èáè * Â§âÂåñÂπÖ"
    # ‰øÆÊ≠£2Ôºöscalar>¬±0.1 „Çí "return ÈñãÂßãÂÄ§ + (abs(scalar) - 0.1(ÁØÑÂõ≤)) / ÁØÑÂõ≤Èáè * Â§âÂåñÂπÖ"
    # „Çø„Çπ„ÇØÁ≠â„Å´Âøú„Åò„ÅüË™øÊï¥„ÅÆ„Åü„ÇÅÔºìÊÆµÈöé„ÅßÈÅ©Áî®„Åó„Å¶„Åä„Åè(‰∏äË®ò„ÇíÂèÇËÄÉ„Å´Ë™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºèÁèæÁä∂„ÅØshadow-effectÂèçÊò†)
    def _decide_ratio(self, scalar):
        if not self.use_shadow:
            return 0.0 # üî∏use_shadow „Åå False „ÅÆÂ†¥Âêà„ÅØÂ∏∏„Å´ÊØîÁéá„Çí 0 „Å´„Åô„Çã
        if abs(scalar) > 0.6:
            return 0.6 + (abs(scalar) - 0.6) / 0.4 * 0.4 # ÂÖÉ return 0.7 + 0.2 * scalar
        elif abs(scalar) > 0.1:
            return 0.1 + (abs(scalar) - 0.1) / 0.5 * 0.5 # ÂÖÉ return 0.3
        return 0.0

    # ÊêçÂ§±ÂèñÂæó(ÊêçÂ§±ÂÄ§ loss_val „ÇíÊï∞ÂÄ§Âåñ„ÄÅÊÑüÊÉÖÂà§ÂÆö„Å´‰ΩøÁî®„ÄÅÂ≠òÂú®„Åó„Å™„ÅÑ„Éë„É©„É°„Éº„Çø(Êõ¥Êñ∞‰∏çË¶Å)„ÅØ„Çπ„Ç≠„ÉÉ„Éó)
    @torch.no_grad()
    def step(self, closure=None):
        loss = closure() if closure is not None else None
        loss_val = loss.item() if loss is not None else 0.0

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]

                # ÊÑüÊÉÖEMAÊõ¥Êñ∞„Éª„Çπ„Ç´„É©„ÉºÁîüÊàê (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ)
                ema = self._update_ema(state, loss_val)
                scalar = self._compute_scalar(ema)
                ratio = self._decide_ratio(scalar) # üî∏use_shadow „Å´Âøú„Åò„Å¶ ratio „Åå 0 „Å´„Å™„Çã

                # shadow_paramÔºöÂøÖË¶ÅÊôÇ„ÅÆ„ÅøÊõ¥Êñ∞ (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ)
                # üî∏self.use_shadow „Åå True „Åß„ÄÅ„Åã„Å§ ratio > 0 „ÅÆÂ†¥Âêà„ÅÆ„Åø shadow „ÇíÊõ¥Êñ∞
                if self.use_shadow and ratio > 0: 
                    if 'shadow' not in state:
                        state['shadow'] = p.clone()
                    else:
                        p.mul_(1 - ratio).add_(state['shadow'], alpha=ratio)
                        state['shadow'].lerp_(p, 0.05)
                
                # --- ÂãæÈÖçË£úÊ≠£„É≠„Ç∏„ÉÉ„ÇØ ---
                # Ë°åÂàó„ÅÆÂΩ¢Áä∂„Åå2Ê¨°ÂÖÉ‰ª•‰∏ä„ÅÆÂ†¥Âêà„ÄÅÂàÜÊï£ÊÉÖÂ†±„Éô„Éº„Çπ„ÅÆABËøë‰ºº„Çí‰ΩøÁî®
                if grad.dim() >= 2:
                    # Ë°å„Å®Âàó„ÅÆ2‰πóÂπ≥Âùá„ÇíË®àÁÆó (ÂàÜÊï£„ÅÆËªΩÈáè„Å™Ëøë‰ºº)
                    r_sq = torch.mean(grad * grad, dim=tuple(range(1, grad.dim())), keepdim=True).add_(group['eps'])
                    c_sq = torch.mean(grad * grad, dim=0, keepdim=True).add_(group['eps'])

                    # ÂàÜÊï£ÊÉÖÂ†±„Åã„ÇâÂãæÈÖç„ÅÆËøë‰ººË°åÂàó„ÇíÁîüÊàê
                    # ABË°åÂàó„Å®„Åó„Å¶Ë¶ãÁ´ã„Å¶„Åü„ÇÇ„ÅÆ„ÇíÁõ¥Êé•ÁîüÊàê„ÅóÊõ¥Êñ∞È†Ö„ÇíË®àÁÆó„Åô„Çã
                    # A = sqrt(r_sq), B = sqrt(c_sq) „Å®„Åô„Çã„Åì„Å®„ÅßABË°åÂàó„ÅÆËøë‰ºº„ÇíÂÜçÁèæ
                    # „Åì„Çå„ÇíEMA„ÅßÂπ≥ÊªëÂåñ„Åô„Çã
                    beta1, beta2 = group['betas'] 
                    eps = group['eps'] 
                    lr = group['lr']   
                    exp_avg = state.setdefault('exp_avg', torch.zeros_like(p))
                    blended_grad = grad.mul(1 - beta1).add_(exp_avg, alpha=beta1)
                    grad_norm = torch.norm(grad, dtype=torch.float32)
                    # > abs 0.6 Cautious (ÈÅéÈÅ©Âêà„ÇÑÂ¥©Â£äÂÇæÂêë„ÇíÊÖéÈáç„Å´)
                    # > abs 0.1 SoftSign+NormEPS (Êè∫„Çå„ÇíÊªë„Çâ„Åã„Å´)
                    # ÂâäÈô§Ôºö„Åù„Çå‰ª•Â§ñ SoftSign („ÇÜ„Å£„Åè„ÇäÊªë„Çâ„Åã„Å´)
                    # p - lr * softsign(blended_grad) (from softsign)
                    # p - lr * direction * mask (from Cautious)
                    # safe_norm Ê•µÂÄ§„ÅÆ„Éñ„É¨„É≥„ÉâÂãæÈÖç„Å´ÂØæ„Åô„Çã„Çπ„Ç±„Éº„É™„É≥„Ç∞
                    if abs(scalar) > 0.6:
                        direction = blended_grad.sign()    # ÂãæÈÖçÊñπÂêë„ÅÆÁ¨¶Âè∑ Cautious Âá¶ÁêÜ
                        mask = (direction == grad.sign())  # ÈÅéÂéª„ÅÆÂãæÈÖç„Å®ÊñπÂêë„Åå‰∏ÄËá¥„Åô„ÇãÈÉ®ÂàÜ„ÅÆ„ÅøÊõ¥Êñ∞
                        p.add_(direction * mask, alpha = -lr)  # Cautious Êõ¥Êñ∞
                    elif abs(scalar) > 0.1:
                        safe_norm = grad_norm + eps
                        modified_grad = softsign(blended_grad) * safe_norm
                        p.add_(-lr * modified_grad) 
                    
                    state.setdefault('exp_avg_r', torch.zeros_like(r_sq)).mul_(beta1).add_(torch.sqrt(r_sq), alpha=1 - beta1)
                    state.setdefault('exp_avg_c', torch.zeros_like(c_sq)).mul_(beta1).add_(torch.sqrt(c_sq), alpha=1 - beta1)
                    
                    # ÂÜçÊßãÁØâ„Åó„ÅüËøë‰ººÂãæÈÖç„ÅÆÂπ≥ÊñπÊ†π„ÅÆÁ©ç„ÅßÊ≠£Ë¶èÂåñ
                    # „Åì„Çå„Å´„Çà„Çä2Ê¨°„É¢„Éº„É°„É≥„Éà„ÅÆ„Çà„ÅÜ„Å™ÂΩπÂâ≤„ÇíÊûú„Åü„Åô
                    denom = torch.sqrt(state['exp_avg_r'] * state['exp_avg_c']) + eps
                    
                    # ÊúÄÁµÇÁöÑ„Å™Êõ¥Êñ∞È†Ö„ÇíË®àÁÆó
                    update_term = grad / denom

                # 1Ê¨°ÂÖÉ(„Éô„ÇØ„Éà„É´)„ÅÆÂãæÈÖçË£úÊ≠£(decoupled weight decay ÊßãÈÄ†„Å´Ëøë„ÅÑ)
                else:
                    exp_avg = state.setdefault('exp_avg', torch.zeros_like(p))
                    exp_avg_sq = state.setdefault('exp_avg_sq', torch.zeros_like(p))
                    beta1, beta2 = group['betas']
                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                    denom = exp_avg_sq.sqrt().add_(group['eps'])
                    update_term = exp_avg / denom

                # ÊúÄÁµÇÁöÑ„Å™„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞ (decoupled weight decay„ÇÇÈÅ©Áî®)
                p.add_(p, alpha=-group['weight_decay'] * group['lr'])
                p.add_(update_term, alpha=-group['lr'])

                # --- Early Stop „É≠„Ç∏„ÉÉ„ÇØ (Êó¢Â≠ò„É≠„Ç∏„ÉÉ„ÇØ„ÇíÁ∂≠ÊåÅ) ---
                hist = self.state.setdefault('scalar_hist', [])
                hist.append(scalar)
                if len(hist) >= 33:
                    hist.pop(0)

        # Early StopÂà§Êñ≠
        if len(self.state['scalar_hist']) >= 32:
            buf = self.state['scalar_hist']
            avg_abs = sum(abs(s) for s in buf) / len(buf)
            std = sum((s - sum(buf)/len(buf))**2 for s in buf) / len(buf)
            if avg_abs < 0.05 and std < 0.005:
                self.should_stop = True

        return loss

"""
 https://github.com/muooon/EmoNavi
 Zeal is inspired by Adafactor, and EmoFact,  
 and its VRAM-friendly design is something everyone loves.
"""